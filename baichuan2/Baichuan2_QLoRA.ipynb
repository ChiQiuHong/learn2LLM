{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baichuan2-7B QLoRA微调实战\n",
    "\n",
    "信息抽取数据集采自CCF2022工业知识图谱关系抽取比赛，并针对一个工业制造领域的相关故障文本，抽取4种类型的实体（部件单元、性能表征、故障状态和检测工具）以及4种类型的关系（部件故障、性能故障、检测工具、组成）。由于数据量较少，在实战过程中，随机抽取50条数据作为测试数据，其余数据作为训练数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Baichuan2-7B-Chat 预训练模型\n",
    "\n",
    "经实测，在线量化8bits会占用大量的内存并且速度缓慢，因此采用离线量化，将8bits量化模型保存下来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = Namespace()\n",
    "\n",
    "# model\n",
    "cfg.model_name_or_path = \"/mnt/d/models/Baichuan2-7B-Chat-8bits\"\n",
    "cfg.train_path = \"/home/huang/codespace/learn2LLM/dataset/fine-tuning/spo_0.json\"\n",
    "cfg.test_path = \"/home/huang/codespace/learn2LLM/dataset/fine-tuning/spo_1.json\"\n",
    "cfg.num_return_sequences = 1\n",
    "cfg.max_len = 1024\n",
    "cfg.max_src_len = 640\n",
    "cfg.batch_size = 2\n",
    "cfg.prompt_text = \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "modelpath = \"/mnt/d/models/Baichuan2-7B-Chat\"\n",
    "quant8_saved_dir = \"/mnt/d/models/Baichuan2-7B-Chat-8bits\"\n",
    "\n",
    "# 保存 8bits量化模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(modelpath, load_in_8bit=True, device_map=\"cpu\", trust_remote_code=True)\n",
    "# model.save_pretrained(quant8_saved_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型保存后，也要将其余的文件复制到quant8_saved_dir文件中，这里省略了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eb6ee0f057417dac3f2822d97b388b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(quant8_saved_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant8_saved_dir, device_map=\"auto\", trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(quant8_saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  \"温故而知新\"是一个成语，源自于中国古代哲学家和教育家孔子。这个成语的意思是通过回顾和了解过去的事情，从而发现新的知识和道理。它强调了学习和成长的过程应该是一个不断循环和进步的过程，既要学习新知识，也要回顾和巩固旧知识。\n",
      "\n",
      "这个成语可以用来指导我们在日常生活中如何更好地学习和成长。例如，当我们学习一门新课程或者掌握一项新技能时，可以回顾以前学过的相关知识，以便更好地理解和应用新技术。同时，通过不断地学习和实践，我们可以发现新的知识和道理，从而不断提高自己的能力和素质。\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调前的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prompt_text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  性能故障_部件故障_组成, 检测工具_奔腾B70做PDI检查\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": get_prompt(\"故障现象：奔腾B70做PDI检查时车辆无法启动。\")})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from data_set import Seq2SeqDataSet2, coll_fn\n",
    "\n",
    "train_dataset = val_dataset = Seq2SeqDataSet2(cfg.train_path, tokenizer, cfg.max_len, cfg.max_src_len,\n",
    "                               cfg.prompt_text)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=cfg.batch_size,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              collate_fn=coll_fn,\n",
    "                              drop_last=True,\n",
    "                              num_workers=2)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=cfg.batch_size,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=coll_fn,\n",
    "                            num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model.supports_gradient_checkpointing = True  #节约cuda，但可能会使得训练时间变长\n",
    "model.gradient_checkpointing_enable() # 作用同上\n",
    "model.enable_input_require_grads() # 作用同上\n",
    "\n",
    "model.config.use_cache = False  # 关闭了模型的缓存机制，该设置可以避免一些警告，但在模型推理时需要重新开启"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training \n",
    "model = prepare_model_for_kbit_training(model) #预处理量化模型以适配LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'up_proj', 'W_pack', 'gate_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb \n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    找出所有全连接层，为所有全连接添加低秩adapter\n",
    "    \"\"\"\n",
    "    cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "lora_modules = find_all_linear_names(model)\n",
    "\n",
    "print(lora_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26,838,912 || all params: 7,532,812,320 || trainable%: 0.35629338499170254\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules= lora_modules\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(**batch).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#仅仅保存lora可训练参数\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    import os\n",
    "    self.net.load_state_dict(\n",
    "        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处设置is_paged=True，即使用Paged Optimizer，减少训练过程中Cuda OOM的风险。\n",
    "optimizer = bnb.optim.adamw.AdamW(peft_model.parameters(),\n",
    "                                  lr=6e-05,is_paged=True)  \n",
    "\n",
    "\n",
    "keras_model = KerasModel(peft_model,loss_fn = None,\n",
    "        optimizer=optimizer) \n",
    "\n",
    "ckpt_path = 'baichuan2_qlora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m<<<<<< ⚡️ cuda is used >>>>>>\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* background: */\n",
       "    progress::-webkit-progress-bar {background-color: #CDCDCD; width: 100%;}\n",
       "    progress {background-color: #CDCDCD;}\n",
       "\n",
       "    /* value: */\n",
       "    progress::-webkit-progress-value {background-color: #00BFFF  !important;}\n",
       "    progress::-moz-progress-bar {background-color: #00BFFF  !important;}\n",
       "    progress {color: #00BFFF ;}\n",
       "\n",
       "    /* optional */\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #000000;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0% [0/5]\n",
       "      <br>\n",
       "      \n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huang/anaconda3/envs/chatglm2/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 12.00 GiB total capacity; 10.88 GiB already allocated; 0 bytes free; 11.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb 单元格 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m keras_model\u001b[39m.\u001b[39;49mfit(train_data \u001b[39m=\u001b[39;49m train_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                 val_data \u001b[39m=\u001b[39;49m val_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                 monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmin\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                 ckpt_path \u001b[39m=\u001b[39;49m ckpt_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                )\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torchkeras/kerasmodel.py:204\u001b[0m, in \u001b[0;36mKerasModel.fit\u001b[0;34m(self, train_data, val_data, epochs, ckpt_path, patience, monitor, mode, callbacks, plot, wandb, quiet, mixed_precision, cpu, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m train_epoch_runner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEpochRunner(train_step_runner,should_quiet)\n\u001b[1;32m    203\u001b[0m train_metrics \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m:epoch}\n\u001b[0;32m--> 204\u001b[0m train_metrics\u001b[39m.\u001b[39mupdate(train_epoch_runner(train_dataloader))\n\u001b[1;32m    206\u001b[0m \u001b[39mfor\u001b[39;00m name, metric \u001b[39min\u001b[39;00m train_metrics\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mget(name, []) \u001b[39m+\u001b[39m [metric]\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torchkeras/kerasmodel.py:77\u001b[0m, in \u001b[0;36mEpochRunner.__call__\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m loop: \n\u001b[1;32m     76\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet):\n\u001b[0;32m---> 77\u001b[0m         step_losses,step_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteprunner(batch)   \n\u001b[1;32m     78\u001b[0m         step_log \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(step_losses,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstep_metrics)\n\u001b[1;32m     79\u001b[0m         \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m step_losses\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;32m/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb 单元格 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m#loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/huang/codespace/learn2LLM/baichuan2/Baichuan2_QLoRA.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/peft/peft_model.py:918\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    909\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    910\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    916\u001b[0m         )\n\u001b[0;32m--> 918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    919\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    920\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    921\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    922\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    923\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    924\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    925\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    926\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    929\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/peft/tuners/adalora.py:299\u001b[0m, in \u001b[0;36mAdaLoraModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 299\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(outputs, \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m         \u001b[39m# Calculate the orthogonal regularization\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         orth_reg_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable_adapter_name]\u001b[39m.\u001b[39morth_reg_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat-8bits/modeling_baichuan.py:686\u001b[0m, in \u001b[0;36mBaichuanForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    685\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    687\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    688\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    689\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    690\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    691\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    692\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    693\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    694\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    695\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    696\u001b[0m )\n\u001b[1;32m    698\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    699\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat-8bits/modeling_baichuan.py:453\u001b[0m, in \u001b[0;36mBaichuanModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39minputs, output_attentions, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    451\u001b[0m         \u001b[39mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 453\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m    454\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    455\u001b[0m         hidden_states,\n\u001b[1;32m    456\u001b[0m         attention_mask,\n\u001b[1;32m    457\u001b[0m         position_ids,\n\u001b[1;32m    458\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    459\u001b[0m     )\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    462\u001b[0m         hidden_states,\n\u001b[1;32m    463\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    467\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m    468\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat-8bits/modeling_baichuan.py:449\u001b[0m, in \u001b[0;36mBaichuanModel.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward\u001b[39m(\u001b[39m*\u001b[39minputs):\n\u001b[1;32m    448\u001b[0m     \u001b[39m# None for past_key_value\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49minputs, output_attentions, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat-8bits/modeling_baichuan.py:286\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    284\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    285\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 286\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    287\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    289\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Baichuan2-7B-Chat-8bits/modeling_baichuan.py:169\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup_proj(x))\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/peft/tuners/adalora.py:512\u001b[0m, in \u001b[0;36mSVDLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 512\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_adapters \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_adapter \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_A\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:441\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m x\u001b[39m.\u001b[39mdtype:\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 441\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mCxB \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         \u001b[39m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    446\u001b[0m         \u001b[39m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:563\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m threshold \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    562\u001b[0m     state\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[0;32m--> 563\u001b[0m \u001b[39mreturn\u001b[39;00m MatMul8bitLt\u001b[39m.\u001b[39;49mapply(A, B, out, bias, state)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chatglm2/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:421\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39m# 4. Mixed-precision decomposition matmul\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m coo_tensorA \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m subA \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 421\u001b[0m     output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(subA, state\u001b[39m.\u001b[39;49msubB)\n\u001b[1;32m    423\u001b[0m \u001b[39m# 5. Save state\u001b[39;00m\n\u001b[1;32m    424\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 12.00 GiB total capacity; 10.88 GiB already allocated; 0 bytes free; 11.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "keras_model.fit(train_data = train_dataloader,\n",
    "                val_data = val_dataloader,\n",
    "                epochs=5, patience=5,\n",
    "                monitor='val_loss', mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
