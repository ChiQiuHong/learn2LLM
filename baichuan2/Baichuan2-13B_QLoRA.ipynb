{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d105df77-a06e-4beb-a1a5-6a2b18b19848",
   "metadata": {},
   "source": [
    "# Baichuan2-13B QLoRA微调 ModelScope\n",
    "\n",
    "使用了ModelScope平台 \n",
    "\n",
    "QLoRA微调：模型本身用4bit加载，训练时把数值反量化到bf16后进行训练，利用LoRA可以锁定原模型参数不参与训练，只训练少量LoRA参数的特性使得训练所需的显存大大减少。\n",
    "\n",
    "前期工作都准备好后，可以跳到 `1.2 编写Dataset` 运行\n",
    "\n",
    "## 安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a3b2c5-a9c5-4d10-b4f8-a2158e73923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate\n",
    "!pip install -q peft\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b1b8a-8f97-4f6f-a3b5-bfbef7e8f8a9",
   "metadata": {},
   "source": [
    "## 0 Baichuan2-13B-Chat 4bits量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890e68c-6498-419a-8f44-0a2ce824436d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('baichuan-inc/baichuan2-13B-Chat', cache_dir='baichuan2-13B-Chat', revision='v1.0.2')\n",
    "\n",
    "# model_dir = snapshot_download('baichuan-inc/baichuan2-13B-Chat-4bits', cache_dir='baichuan2-13B-Chat-4bits', revision='v1.0.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3503c88-2e41-4902-916e-f990a77b53a6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "model_path = \"baichuan2-13B-Chat-4bits/baichuan-inc/baichuan2-13B-Chat-4bits\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\", trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "# 约占用9.7GB的显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1894569-997d-4831-b337-d8555a5a99a4",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T06:03:41.744062Z",
     "iopub.status.busy": "2023-09-22T06:03:41.743533Z",
     "iopub.status.idle": "2023-09-22T06:03:49.508746Z",
     "shell.execute_reply": "2023-09-22T06:03:49.508162Z",
     "shell.execute_reply.started": "2023-09-22T06:03:41.744036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  “温故而知新”是一句源自《论语·为政》的古文名言，它意味着通过回顾过去的学习和经验，从而获得新的理解和领悟。这句话鼓励我们在学习过程中不断地复习和巩固已学知识，同时尝试从中发现新的观点和想法。\n",
      "\n",
      "具体来说，“温故”是指回顾过去的知识和经验，而“知新”则是指在回顾的过程中发现新的理解和启示。这句名言鼓励我们既要珍惜过去的成果，也要勇于探索未知的新领域，从而实现持续的成长和发展。\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)\n",
    "\n",
    "# 推理约占用13GB显存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab96902-0298-4505-87b9-9deb4521d9b8",
   "metadata": {},
   "source": [
    "### 微调前的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41dba4a1-b7fb-4055-8975-ee2880a09500",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-23T03:19:05.917667Z",
     "iopub.status.busy": "2023-09-23T03:19:05.917313Z",
     "iopub.status.idle": "2023-09-23T03:19:05.920780Z",
     "shell.execute_reply": "2023-09-23T03:19:05.920246Z",
     "shell.execute_reply.started": "2023-09-23T03:19:05.917646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_text = \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prompt_text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac6077b-5f0e-4768-89f8-bbe5f37fc319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T06:06:44.131029Z",
     "iopub.status.busy": "2023-09-22T06:06:44.130666Z",
     "iopub.status.idle": "2023-09-22T06:06:45.310978Z",
     "shell.execute_reply": "2023-09-22T06:06:45.310304Z",
     "shell.execute_reply.started": "2023-09-22T06:06:44.131008Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  性能故障_奔腾B70; 部件故障_PDI检查; 组成_无法启动\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": get_prompt(\"故障现象：奔腾B70做PDI检查时车辆无法启动。\")})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8f635-4186-49dd-a5d9-c87cd106ed09",
   "metadata": {},
   "source": [
    "## 1 加载训练数据\n",
    "\n",
    "### 1.1 准备训练数据\n",
    "首先，需要准备训练数据，需要将所有样本放到列表中并存入json文件中。每个样本对应一个字典，包含id和conversations，其中后者为一个列表。示例如下所示：\n",
    "\n",
    "```JSON\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\\n故障现象：奔腾B70做PDI检查时车辆无法启动。\",\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"车辆_部件故障_无法启动\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57bbc77-5fab-4648-91a8-65a9d6e4587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def ftdata_process(ori_path, train_path, test_path):\n",
    "    data = []\n",
    "    with open(ori_path, 'r', encoding='utf-8') as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            sample = json.loads(line.strip())\n",
    "            conversations = []\n",
    "            text = sample[\"text\"]\n",
    "            new_entry = {\n",
    "                \"id\": sample['ID'],\n",
    "                \"conversations\": conversations\n",
    "            }\n",
    "            \n",
    "            # 创建\"user\"输入\n",
    "            user_input = {\n",
    "                \"from\": \"user\",\n",
    "                \"value\": f'你现在是一个信息抽取模型，请你帮我抽取出关系内容为\"性能故障\", \"部件故障\", \"组成\"和 \"检测工具\"的相关三元组，三元组内部用\"_\"连接，三元组之间用\\\\n分割。文本：\\\\n{text}'\n",
    "            }\n",
    "            conversations.append(user_input)\n",
    "            \n",
    "            # 创建\"assistant\"回应\n",
    "            spo_list = []\n",
    "            for spo in sample['spo_list']:\n",
    "                spo_list.append('_'.join([spo['h'][\"name\"], spo['relation'], spo['t']['name']]))\n",
    "            assistant_response = {\n",
    "                \"from\": \"assistant\",\n",
    "                \"value\": \"\\\\n\".join(spo_list)\n",
    "            }\n",
    "            conversations.append(assistant_response)\n",
    "            \n",
    "            data.append(new_entry)\n",
    "\n",
    "    # 随机抽取50条数据作为测试集\n",
    "    test_set = random.sample(data, min(50, len(data)))\n",
    "    train_set = [record for record in data if record not in test_set]\n",
    "\n",
    "    with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_set, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_set, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16fa12-9d37-4054-975c-e7175a5dc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_path = \"dataset/ori_data.json\"\n",
    "train_path = \"dataset/train.json\"\n",
    "test_path = \"dataset/test.json\"\n",
    "ftdata_process(ori_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb2a82f-aed9-4bc0-a941-3630379907ad",
   "metadata": {},
   "source": [
    "### 1.2 编写Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051d29e1-c493-4c8e-b3ae-aa843cab3ba1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T07:25:45.171511Z",
     "iopub.status.busy": "2023-09-22T07:25:45.171329Z",
     "iopub.status.idle": "2023-09-22T07:25:46.319977Z",
     "shell.execute_reply": "2023-09-22T07:25:46.319383Z",
     "shell.execute_reply.started": "2023-09-22T07:25:45.171493Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources, \n",
    "    tokenizer, \n",
    "    max_len, \n",
    ") -> Dict:\n",
    "    user_tokens=[195]\n",
    "    assistant_tokens=[196]\n",
    "    ignore_index = -100\n",
    "\n",
    "    input_ids, labels = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        input_id, label = [], []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = sentence[\"from\"]\n",
    "            value = sentence[\"value\"]\n",
    "            value_ids = tokenizer.encode(value)\n",
    "\n",
    "            if role == \"user\":\n",
    "                input_id += user_tokens + value_ids\n",
    "                label += [tokenizer.eos_token_id] + [ignore_index] * len(value_ids)\n",
    "            else:\n",
    "                input_id += assistant_tokens + value_ids\n",
    "                label += [ignore_index] + value_ids\n",
    "        assert len(input_id) == len(label)\n",
    "        input_id.append(tokenizer.eos_token_id)\n",
    "        label.append(tokenizer.eos_token_id)\n",
    "        input_id = input_id[:max_len]\n",
    "        label = label[:max_len]\n",
    "        \n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        label += [ignore_index] * (max_len - len(label))\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    def __init__(self, raw_data, tokenizer, max_len):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer, self.max_len)\n",
    "        ret =  dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4db141f3-f5a4-495c-bd0e-8b42466092e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T07:25:46.321289Z",
     "iopub.status.busy": "2023-09-22T07:25:46.320962Z",
     "iopub.status.idle": "2023-09-22T07:25:46.325335Z",
     "shell.execute_reply": "2023-09-22T07:25:46.324820Z",
     "shell.execute_reply.started": "2023-09-22T07:25:46.321270Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_supervised_data_module(\n",
    "    tokenizer, cfg, max_len,\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = SupervisedDataset\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(cfg.train_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "    if cfg.eval_path:\n",
    "        eval_json = json.load(open(cfg.eval_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459fecd-b31a-40f3-adb4-1e0a13c8df30",
   "metadata": {},
   "source": [
    "## 2 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452028e1-a1e4-41d0-af83-8d102c58292e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T07:26:20.574417Z",
     "iopub.status.busy": "2023-09-22T07:26:20.573831Z",
     "iopub.status.idle": "2023-09-22T07:26:20.580286Z",
     "shell.execute_reply": "2023-09-22T07:26:20.579728Z",
     "shell.execute_reply.started": "2023-09-22T07:26:20.574390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from argparse import Namespace\n",
    "cfg = Namespace()\n",
    "\n",
    "# model\n",
    "# cfg.model_name_or_path = 'baichuan2-13B-Chat-4bits/baichuan-inc/baichuan2-13B-Chat-4bits'\n",
    "cfg.model_name_or_path = \"baichuan2-13B-Chat/baichuan-inc/baichuan2-13B-Chat\"\n",
    "cfg.train_path = \"dataset/train.json\"\n",
    "cfg.eval_path = None\n",
    "cfg.test_path = \"dataset/test.json\"\n",
    "cfg.model_max_length = 768\n",
    "cfg.batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"baichuan2-13B-chat_QLoRA\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=cfg.batch_size,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4507f-bc61-4405-b7ee-f43b015f5e60",
   "metadata": {},
   "source": [
    "BitsAndBytes 支持 8bits 和 4bits 两种量化，其中 4bits 支持 FP4 和 NF4 两种格式，Baichuan 2 选用 NF4 作为 4bits 量化的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956440be-074c-4b6e-b4c5-9d7b78a1115c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T07:26:23.570137Z",
     "iopub.status.busy": "2023-09-22T07:26:23.569766Z",
     "iopub.status.idle": "2023-09-22T07:26:23.574620Z",
     "shell.execute_reply": "2023-09-22T07:26:23.574093Z",
     "shell.execute_reply.started": "2023-09-22T07:26:23.570113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47429e12-64ff-40f0-9e54-49b7a39b60a6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T07:26:26.277665Z",
     "iopub.status.busy": "2023-09-22T07:26:26.277033Z",
     "iopub.status.idle": "2023-09-22T07:29:11.715960Z",
     "shell.execute_reply": "2023-09-22T07:29:11.715397Z",
     "shell.execute_reply.started": "2023-09-22T07:26:26.277645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d1addef216467faf8e2d1c5bc8c842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=cfg.model_max_length,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.supports_gradient_checkpointing = True  #节约cuda，但可能会使得训练时间变长\n",
    "model.gradient_checkpointing_enable() # 作用同上\n",
    "model.enable_input_require_grads() # 作用同上\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18d8797-e742-46fb-b044-43e60acf1334",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T06:59:10.667579Z",
     "iopub.status.busy": "2023-09-22T06:59:10.666921Z",
     "iopub.status.idle": "2023-09-22T06:59:10.673088Z",
     "shell.execute_reply": "2023-09-22T06:59:10.672478Z",
     "shell.execute_reply.started": "2023-09-22T06:59:10.667559Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down_proj', 'up_proj', 'W_pack', 'gate_proj', 'o_proj']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb \n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    找出所有全连接层\n",
    "    \"\"\"\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "lora_modules = find_all_linear_names(model)\n",
    "print(lora_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c9bb17-205c-4804-8d5e-e21f91cfa39f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-22T07:29:59.891895Z",
     "iopub.status.busy": "2023-09-22T07:29:59.891540Z",
     "iopub.status.idle": "2023-09-22T07:30:53.722328Z",
     "shell.execute_reply": "2023-09-22T07:30:53.721714Z",
     "shell.execute_reply.started": "2023-09-22T07:29:59.891873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 126,812,160 || all params: 14,023,480,320 || trainable%: 0.9042845078845592\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['W_pack', 'o_proj'],\n",
    "    inference_mode=False,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d3c02-4a95-4035-b2b7-fa225188a244",
   "metadata": {},
   "source": [
    "## 3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "279c6e4d-d250-4bcd-8dd0-d55953b5aa86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T07:31:05.223996Z",
     "iopub.status.busy": "2023-09-22T07:31:05.223489Z",
     "iopub.status.idle": "2023-09-22T07:31:05.243188Z",
     "shell.execute_reply": "2023-09-22T07:31:05.242638Z",
     "shell.execute_reply.started": "2023-09-22T07:31:05.223969Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Formatting inputs...Skip in lazy mode\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, cfg=cfg, max_len=cfg.model_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb96bc58-ede5-44d4-9c4e-d2163adecab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T07:31:08.257461Z",
     "iopub.status.busy": "2023-09-22T07:31:08.257072Z",
     "iopub.status.idle": "2023-09-22T07:31:08.263830Z",
     "shell.execute_reply": "2023-09-22T07:31:08.263112Z",
     "shell.execute_reply.started": "2023-09-22T07:31:08.257438Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, tokenizer=tokenizer, **data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e20b133-0a12-462b-8859-31148786e7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-22T07:31:09.458295Z",
     "iopub.status.busy": "2023-09-22T07:31:09.457914Z",
     "iopub.status.idle": "2023-09-22T19:29:14.271420Z",
     "shell.execute_reply": "2023-09-22T19:29:14.268739Z",
     "shell.execute_reply.started": "2023-09-22T07:31:09.458273Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3605' max='3605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3605/3605 11:57:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.176600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (889 > 768). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3605, training_loss=0.301301208051663, metrics={'train_runtime': 43084.596, 'train_samples_per_second': 0.167, 'train_steps_per_second': 0.084, 'total_flos': 2.34906267746304e+17, 'train_loss': 0.301301208051663, 'epoch': 5.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31685893-2142-4544-acfc-82fdad0ca450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T02:52:12.310767Z",
     "iopub.status.busy": "2023-09-23T02:52:12.310418Z",
     "iopub.status.idle": "2023-09-23T02:52:12.740031Z",
     "shell.execute_reply": "2023-09-23T02:52:12.739461Z",
     "shell.execute_reply.started": "2023-09-23T02:52:12.310746Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=\"baichuan2-13B-chat_qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e12b0-76a1-454c-8040-92cbd39fce98",
   "metadata": {},
   "source": [
    "## 4 验证训练后结果\n",
    "\n",
    "因为显存和内存都不够，所以用8bits量化，显存占用约16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054b5b97-9b58-45ba-b8c3-ac42a49cbdeb",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-23T03:14:09.579749Z",
     "iopub.status.busy": "2023-09-23T03:14:09.579106Z",
     "iopub.status.idle": "2023-09-23T03:17:03.635575Z",
     "shell.execute_reply": "2023-09-23T03:17:03.634974Z",
     "shell.execute_reply.started": "2023-09-23T03:14:09.579727Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-23 11:14:12,237] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 11:14:15.974271: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-23 11:14:16.013896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-23 11:14:17.106472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e95a12631ff4ff7aad00ae596a2d529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "model_path = \"baichuan2-13B-Chat/baichuan-inc/baichuan2-13B-Chat\"\n",
    "peft_model_path = \"baichuan2-13B-chat_qlora\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a27c0298-5ad8-4232-b469-8dfff74804b9",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-09-23T03:17:18.208409Z",
     "iopub.status.busy": "2023-09-23T03:17:18.208051Z",
     "iopub.status.idle": "2023-09-23T03:18:13.987109Z",
     "shell.execute_reply": "2023-09-23T03:18:13.986498Z",
     "shell.execute_reply.started": "2023-09-23T03:17:18.208389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ckpt_path = 'baichuan2-13B-chat_qlora'\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b17195f-e770-4077-b8c6-935cbf07bfe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T03:19:09.117733Z",
     "iopub.status.busy": "2023-09-23T03:19:09.117392Z",
     "iopub.status.idle": "2023-09-23T03:19:21.622771Z",
     "shell.execute_reply": "2023-09-23T03:19:21.622029Z",
     "shell.execute_reply.started": "2023-09-23T03:19:09.117714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  密封垫_部件故障_密封不严\\n机油_部件故障_压力过高\\n机油_部件故障_渗漏\\n机油_部件故障_上窜进燃烧室\\n气门_部件故障_卡死\\n润滑油_部件故障_消耗过大\\n汽车_部件故障_尾气排出蓝烟\\n零部件_部件故障_磨损\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": get_prompt(\"原因分析：细心的朋友会发现，润滑油在车况良好的情况下也存在正常的消耗，但有些车况较差的时候，汽车的尾气排出蓝烟，其实这就意味润滑油消耗过大，一般来说，润滑油的消耗无非两种情况，进入燃烧室参与燃烧，或是机油渗漏。之所以机油能够窜入燃烧室，主要是因为零部件严重磨损，配合间隙过大，或者机油压力过高，导致机油上窜进燃烧室。而机油的渗漏主要是因为密封垫变硬老化、气门卡死。如果是老旧车辆，一般都存在密封垫由于老化而密封不严的情况。遇到以上情况，您最好是通过专业的养护中心，由养护工程师进行判定，并实施行之有效的解决办法。\")})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20684729-15a8-4ab9-8c4f-eb2794aba802",
   "metadata": {},
   "source": [
    "## 5 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef4a37a-5e9d-4ecd-a0aa-be436cb3579a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T03:20:32.281185Z",
     "iopub.status.busy": "2023-09-23T03:20:32.280666Z",
     "iopub.status.idle": "2023-09-23T03:20:32.285294Z",
     "shell.execute_reply": "2023-09-23T03:20:32.284613Z",
     "shell.execute_reply.started": "2023-09-23T03:20:32.281163Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_one_sample(model, tokenizer, text):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": get_prompt(text)})\n",
    "    with torch.no_grad():\n",
    "        response = model.chat(tokenizer, messages)\n",
    "    \n",
    "    # 对结果按照“\\n”进行分割，获取每个三元组内容\n",
    "    pre_res = list(set([rr for rr in response.split('\\\\n') if len(rr.split(\"_\"))==3]))\n",
    "\n",
    "    return response, pre_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9998166-d15c-48bd-b407-ec056d7d9389",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-23T03:21:48.130106Z",
     "iopub.status.busy": "2023-09-23T03:21:48.129757Z",
     "iopub.status.idle": "2023-09-23T03:24:57.515534Z",
     "shell.execute_reply": "2023-09-23T03:24:57.514780Z",
     "shell.execute_reply.started": "2023-09-23T03:21:48.130086Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 100%|██████████| 50/50 [03:09<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.5722646131407069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = \"dataset/test.json\"\n",
    "save_data = []\n",
    "f1, total = 0.0, 0.0\n",
    "\n",
    "test_json = json.load(open(test_path, \"r\"))\n",
    "sources = [example[\"conversations\"] for example in test_json]\n",
    "\n",
    "for i, source in enumerate(tqdm(sources, desc=\"iter\")):\n",
    "    total += 1\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = sentence[\"from\"]\n",
    "        value = sentence[\"value\"]\n",
    "\n",
    "        if role == \"user\":\n",
    "            text = value\n",
    "        else:\n",
    "            answer = value\n",
    "    \n",
    "    response, pre_res = predict_one_sample(model, tokenizer, text)\n",
    "    real_res = answer.split(\"\\\\n\")\n",
    "    \n",
    "    # 计算预测与真实的F1值\n",
    "    same_res = set(pre_res) & set(real_res)\n",
    "    if len(set(pre_res)) == 0:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p = len(same_res) / len(set(pre_res))\n",
    "    r = len(same_res) / len(set(real_res))\n",
    "    if (p + r) != 0.0:\n",
    "        f = 2 * p * r / (p + r)\n",
    "    else:\n",
    "        f = 0.0\n",
    "    f1 += f \n",
    "    save_data.append(\n",
    "        {\"text\": text, \"ori_answer\": answer, \"gen_answer\": response, \"f1\": f})\n",
    "    \n",
    "print(\"f1:\", f1 / total)\n",
    "save_path = os.path.join('baichuan2-13B-chat_qlora', \"baichuan2-13B-chat_qlora_ft_answer.json\")\n",
    "fin = open(save_path, \"w\", encoding=\"utf-8\")\n",
    "json.dump(save_data, fin, ensure_ascii=False, indent=4)\n",
    "fin.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
