{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db9cde6-ab46-41fd-aafe-1ec17ebdfd22",
   "metadata": {},
   "source": [
    "# Baichuan2-7B LoRA微调 ModelScope\n",
    "\n",
    "使用了ModelScope平台 \n",
    "\n",
    "fp16精度下LoRA微调，需要24GB的显卡\n",
    "\n",
    "前期工作都准备好后，可以跳到 `1.2 编写Dataset` 运行\n",
    "\n",
    "## 安装环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793528f2-6b5f-43a2-bddf-fbf388e3a26d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate\n",
    "!pip install -q peft\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfbe13-eb82-4a86-ab9d-cacf8669ac72",
   "metadata": {},
   "source": [
    "## 0 Baichuan2-7B-Chat 预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679def8-ae2a-4732-9f9f-130816a09585",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('baichuan-inc/baichuan2-7B-Chat', cache_dir='baichuan2-7B-Chat', revision='v1.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a24b2-add8-4dca-9f48-bf9989a2d287",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/mnt/workspace/baichuan-7B/baichuan-inc/baichuan-7B\")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "model_path = \"baichuan2-7B-Chat/baichuan-inc/baichuan2-7B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model.generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "\n",
    "# 用fp16加载，约占用15.3GB的显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fc159e-6d2a-4c73-ae88-eaefeb3420a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T02:07:49.521214Z",
     "iopub.status.busy": "2023-09-21T02:07:49.520925Z",
     "iopub.status.idle": "2023-09-21T02:07:56.085358Z",
     "shell.execute_reply": "2023-09-21T02:07:56.084588Z",
     "shell.execute_reply.started": "2023-09-21T02:07:49.521194Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  \"温故而知新\"是一句中国古代的成语，出自《论语·为政》。这句话的意思是：通过回顾和了解过去的事情，可以从中获得新的知识和启示。这句话强调了学习和知识的重要性，以及通过不断回顾和总结过去的经验来提高自己的重要性。\n",
      "\n",
      "在现代语境中，这句成语可以用来鼓励人们在学习过程中不断回顾和巩固已知的知识，以便更好地理解和掌握新的知识。同时，它也可以用来强调经验和教训对于个人成长和发展的重要性。\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": \"解释一下“温故而知新”\"})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f739c2ad-bb97-47ef-961b-9d8ce1df8a7d",
   "metadata": {},
   "source": [
    "### 微调前的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1abdc4-c306-4bd3-b43d-a2ec86ebb981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T15:16:22.434644Z",
     "iopub.status.busy": "2023-09-21T15:16:22.434289Z",
     "iopub.status.idle": "2023-09-21T15:16:22.438149Z",
     "shell.execute_reply": "2023-09-21T15:16:22.437528Z",
     "shell.execute_reply.started": "2023-09-21T15:16:22.434622Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_text = \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\"\n",
    "\n",
    "def get_prompt(text):\n",
    "    return prompt_text + text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "525dd187-d011-4fa3-9a27-bf83b1fef9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T02:08:07.103844Z",
     "iopub.status.busy": "2023-09-21T02:08:07.103444Z",
     "iopub.status.idle": "2023-09-21T02:08:07.517215Z",
     "shell.execute_reply": "2023-09-21T02:08:07.516536Z",
     "shell.execute_reply.started": "2023-09-21T02:08:07.103826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  性能故障_部件故障_组成_检测工具\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": get_prompt(\"故障现象：奔腾B70做PDI检查时车辆无法启动。\")})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca9477-e101-43f1-832a-f0cc746c0f86",
   "metadata": {},
   "source": [
    "## 1 加载训练数据\n",
    "\n",
    "### 1.1 准备训练数据\n",
    "首先，需要准备训练数据，需要将所有样本放到列表中并存入json文件中。每个样本对应一个字典，包含id和conversations，其中后者为一个列表。示例如下所示：\n",
    "\n",
    "```JSON\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你现在是一个信息抽取模型，请你帮我抽取出关系内容为\\\"性能故障\\\", \\\"部件故障\\\", \\\"组成\\\"和 \\\"检测工具\\\"的相关三元组，三元组内部用\\\"_\\\"连接，三元组之间用\\\\n分割。文本：\\n故障现象：奔腾B70做PDI检查时车辆无法启动。\",\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"车辆_部件故障_无法启动\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "910876f2-5b8c-404f-a7aa-3a2db828f209",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T03:10:00.044308Z",
     "iopub.status.busy": "2023-09-21T03:10:00.043948Z",
     "iopub.status.idle": "2023-09-21T03:10:00.051183Z",
     "shell.execute_reply": "2023-09-21T03:10:00.050592Z",
     "shell.execute_reply.started": "2023-09-21T03:10:00.044288Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def ftdata_process(ori_path, train_path, test_path):\n",
    "    data = []\n",
    "    with open(ori_path, 'r', encoding='utf-8') as fh:\n",
    "        for i, line in enumerate(fh):\n",
    "            sample = json.loads(line.strip())\n",
    "            conversations = []\n",
    "            text = sample[\"text\"]\n",
    "            new_entry = {\n",
    "                \"id\": sample['ID'],\n",
    "                \"conversations\": conversations\n",
    "            }\n",
    "            \n",
    "            # 创建\"user\"输入\n",
    "            user_input = {\n",
    "                \"from\": \"user\",\n",
    "                \"value\": f'你现在是一个信息抽取模型，请你帮我抽取出关系内容为\"性能故障\", \"部件故障\", \"组成\"和 \"检测工具\"的相关三元组，三元组内部用\"_\"连接，三元组之间用\\\\n分割。文本：\\\\n{text}'\n",
    "            }\n",
    "            conversations.append(user_input)\n",
    "            \n",
    "            # 创建\"assistant\"回应\n",
    "            spo_list = []\n",
    "            for spo in sample['spo_list']:\n",
    "                spo_list.append('_'.join([spo['h'][\"name\"], spo['relation'], spo['t']['name']]))\n",
    "            assistant_response = {\n",
    "                \"from\": \"assistant\",\n",
    "                \"value\": \"\\\\n\".join(spo_list)\n",
    "            }\n",
    "            conversations.append(assistant_response)\n",
    "            \n",
    "            data.append(new_entry)\n",
    "\n",
    "    # 随机抽取50条数据作为测试集\n",
    "    test_set = random.sample(data, min(50, len(data)))\n",
    "    train_set = [record for record in data if record not in test_set]\n",
    "\n",
    "    with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(test_set, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_set, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f026c18-b5c3-45b0-bfc8-4acf81434f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T03:10:07.138423Z",
     "iopub.status.busy": "2023-09-21T03:10:07.138076Z",
     "iopub.status.idle": "2023-09-21T03:10:07.194270Z",
     "shell.execute_reply": "2023-09-21T03:10:07.193780Z",
     "shell.execute_reply.started": "2023-09-21T03:10:07.138400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ori_path = \"dataset/ori_data.json\"\n",
    "train_path = \"dataset/train.json\"\n",
    "test_path = \"dataset/test.json\"\n",
    "ftdata_process(ori_path, train_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3248f-90b7-4733-8f99-8890741d3384",
   "metadata": {},
   "source": [
    "### 1.2 编写Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a7ee00-ae20-42c7-9696-8e5f3567985c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:13:25.020334Z",
     "iopub.status.busy": "2023-09-21T15:13:25.019699Z",
     "iopub.status.idle": "2023-09-21T15:13:25.030771Z",
     "shell.execute_reply": "2023-09-21T15:13:25.030132Z",
     "shell.execute_reply.started": "2023-09-21T15:13:25.020313Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources, \n",
    "    tokenizer, \n",
    "    max_len, \n",
    ") -> Dict:\n",
    "    user_tokens=[195]\n",
    "    assistant_tokens=[196]\n",
    "    ignore_index = -100\n",
    "\n",
    "    input_ids, labels = [], []\n",
    "    for i, source in enumerate(sources):\n",
    "        input_id, label = [], []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = sentence[\"from\"]\n",
    "            value = sentence[\"value\"]\n",
    "            value_ids = tokenizer.encode(value)\n",
    "\n",
    "            if role == \"user\":\n",
    "                input_id += user_tokens + value_ids\n",
    "                label += [tokenizer.eos_token_id] + [ignore_index] * len(value_ids)\n",
    "            else:\n",
    "                input_id += assistant_tokens + value_ids\n",
    "                label += [ignore_index] + value_ids\n",
    "        assert len(input_id) == len(label)\n",
    "        input_id.append(tokenizer.eos_token_id)\n",
    "        label.append(tokenizer.eos_token_id)\n",
    "        input_id = input_id[:max_len]\n",
    "        label = label[:max_len]\n",
    "        \n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        label += [ignore_index] * (max_len - len(label))\n",
    "        \n",
    "        input_ids.append(input_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int)\n",
    "    labels = torch.tensor(labels, dtype=torch.int)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "    def __init__(self, raw_data, tokenizer, max_len):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        \n",
    "        print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer, max_len)\n",
    "        \n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "        \n",
    "        print(\"input:\", tokenizer.decode(self.input_ids[66]))\n",
    "        labels = []\n",
    "        for id_ in self.labels[66]:\n",
    "            if id_ == -100:\n",
    "                continue\n",
    "\n",
    "            labels.append(id_)\n",
    "        print(\"label:\", tokenizer.decode(labels))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e15d9b-9274-4534-a0cc-fecb2839f058",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T12:38:38.997014Z",
     "iopub.status.busy": "2023-09-21T12:38:38.996670Z",
     "iopub.status.idle": "2023-09-21T12:38:39.001303Z",
     "shell.execute_reply": "2023-09-21T12:38:39.000768Z",
     "shell.execute_reply.started": "2023-09-21T12:38:38.996991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_supervised_data_module(\n",
    "    tokenizer, cfg, max_len,\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = SupervisedDataset\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    train_json = json.load(open(cfg.train_path, \"r\"))\n",
    "    train_dataset = dataset_cls(train_json, tokenizer=tokenizer, max_len=max_len)\n",
    "\n",
    "    if cfg.eval_path:\n",
    "        eval_json = json.load(open(cfg.eval_path, \"r\"))\n",
    "        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer, max_len=max_len)\n",
    "    else:\n",
    "        eval_dataset = None\n",
    "\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a1ee5-e60f-4642-abd7-1e631bf1b55d",
   "metadata": {},
   "source": [
    "## 2 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758c503c-9d87-4992-8e19-13e6ac3e657f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:12:59.624264Z",
     "iopub.status.busy": "2023-09-21T15:12:59.623869Z",
     "iopub.status.idle": "2023-09-21T15:13:16.241992Z",
     "shell.execute_reply": "2023-09-21T15:13:16.241370Z",
     "shell.execute_reply.started": "2023-09-21T15:12:59.624238Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 23:13:02.770505: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-21 23:13:03.178651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 23:13:04.400062: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-21 23:13:07,303] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from argparse import Namespace\n",
    "cfg = Namespace()\n",
    "\n",
    "# model\n",
    "cfg.model_name_or_path = 'baichuan2-7B-Chat/baichuan-inc/baichuan2-7B-Chat'\n",
    "cfg.train_path = \"dataset/train.json\"\n",
    "cfg.eval_path = None\n",
    "cfg.test_path = \"dataset/test.json\"\n",
    "cfg.model_max_length = 1024\n",
    "cfg.batch_size = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"baichuan2-7B-chat_lora\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=cfg.batch_size,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbde8209-a6ff-4f89-8f49-29980173b37e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:13:31.598347Z",
     "iopub.status.busy": "2023-09-21T15:13:31.597705Z",
     "iopub.status.idle": "2023-09-21T15:15:02.689774Z",
     "shell.execute_reply": "2023-09-21T15:15:02.689216Z",
     "shell.execute_reply.started": "2023-09-21T15:13:31.598325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce688c6ed18641bf84e8b55893d6c615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=cfg.model_max_length,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381babd8-eb3d-4366-8508-07a917fc864a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T07:38:32.060680Z",
     "iopub.status.busy": "2023-09-21T07:38:32.060315Z",
     "iopub.status.idle": "2023-09-21T07:38:54.638664Z",
     "shell.execute_reply": "2023-09-21T07:38:54.638057Z",
     "shell.execute_reply.started": "2023-09-21T07:38:32.060659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 7,510,167,552 || trainable%: 0.055848341211549045\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"W_pack\"],\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model.supports_gradient_checkpointing = True  #节约cuda，但可能会使得训练时间变长\n",
    "model.gradient_checkpointing_enable() # 作用同上\n",
    "model.enable_input_require_grads() # 作用同上\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2404117-ea25-4065-b248-7652af67635c",
   "metadata": {},
   "source": [
    "## 3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aea194a4-8dbe-435b-b848-7821d8158e69",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T07:43:18.123854Z",
     "iopub.status.busy": "2023-09-21T07:43:18.123432Z",
     "iopub.status.idle": "2023-09-21T07:43:19.434100Z",
     "shell.execute_reply": "2023-09-21T07:43:19.433556Z",
     "shell.execute_reply.started": "2023-09-21T07:43:18.123830Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Formatting inputs...\n",
      "input:  <reserved_106>你现在是一个信息抽取模型，请你帮我抽取出关系内容为\"性能故障\", \"部件故障\", \"组成\"和 \"检测工具\"的相关三元组，三元组内部用\"_\"连接，三元组之间用\\n分割。文本：\\n处理原则：检查监控系统告警信息，相关电流指示；检查各个采样系统的电流测量值有无异常；检查本体有无异常声响、有无异常振动；检查二次回路、电子模块装置和端子接线排有无放电打火、开路现象，查找开路点；二次回路开路，应尽快处理；如不能恢复，应立即汇报值班调控人员申请停运接地极或调整直流系统运行方式；查找零磁通电流互感器二次开路点时应注意安全，应穿绝缘靴，戴绝缘手套，至少两人一起。<reserved_107>二次回路_部件故障_开路</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "label: </s> 二次回路_部件故障_开路</s>\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, cfg=cfg, max_len=cfg.model_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24ccd3f-2a39-44a6-a7c7-ee02eaf2b1ab",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T07:43:26.799520Z",
     "iopub.status.busy": "2023-09-21T07:43:26.798901Z",
     "iopub.status.idle": "2023-09-21T07:43:26.809640Z",
     "shell.execute_reply": "2023-09-21T07:43:26.809003Z",
     "shell.execute_reply.started": "2023-09-21T07:43:26.799496Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args, tokenizer=tokenizer, **data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d17dbe-0921-405e-a315-54a471585934",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T07:43:31.187092Z",
     "iopub.status.busy": "2023-09-21T07:43:31.186442Z",
     "iopub.status.idle": "2023-09-21T09:51:19.769932Z",
     "shell.execute_reply": "2023-09-21T09:51:19.768624Z",
     "shell.execute_reply.started": "2023-09-21T07:43:31.187068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3605' max='3605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3605/3605 2:07:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.331100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3605, training_loss=0.3713475157252297, metrics={'train_runtime': 7668.3308, 'train_samples_per_second': 0.94, 'train_steps_per_second': 0.47, 'total_flos': 3.096653235172147e+17, 'train_loss': 0.3713475157252297, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7094da1-4e38-4894-8ce5-0951a7f256c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-21T10:04:07.182205Z",
     "iopub.status.busy": "2023-09-21T10:04:07.181732Z",
     "iopub.status.idle": "2023-09-21T10:04:07.210892Z",
     "shell.execute_reply": "2023-09-21T10:04:07.210286Z",
     "shell.execute_reply.started": "2023-09-21T10:04:07.182182Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=\"baichuan2-7B-chat_lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77803f22-2d05-4806-bf15-2d83df4ad465",
   "metadata": {},
   "source": [
    "## 4 验证训练后结果\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb04d01e-2a68-4318-b3c0-ebe336120733",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:17:58.254303Z",
     "iopub.status.busy": "2023-09-21T15:17:58.253938Z",
     "iopub.status.idle": "2023-09-21T15:18:32.105572Z",
     "shell.execute_reply": "2023-09-21T15:18:32.104963Z",
     "shell.execute_reply.started": "2023-09-21T15:17:58.254281Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ckpt_path = 'baichuan2-7B-chat_lora/checkpoint-3500'\n",
    "\n",
    "model = PeftModel.from_pretrained(model, ckpt_path)\n",
    "model = model.merge_and_unload() #合并lora权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8635fad1-4fb2-4228-bdb8-48c67eadef5e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:19:01.402168Z",
     "iopub.status.busy": "2023-09-21T15:19:01.401705Z",
     "iopub.status.idle": "2023-09-21T15:19:01.698642Z",
     "shell.execute_reply": "2023-09-21T15:19:01.697392Z",
     "shell.execute_reply.started": "2023-09-21T15:19:01.402147Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baichuan2:  机油_部件故障_泄漏\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"user\", \"content\": get_prompt(\"原因分析：细心的朋友会发现，润滑油在车况良好的情况下也存在正常的消耗，但有些车况较差的时候，汽车的尾气排出蓝烟，其实这就意味润滑油消耗过大，一般来说，润滑油的消耗无非两种情况，进入燃烧室参与燃烧，或是机油渗漏。之所以机油能够窜入燃烧室，主要是因为零部件严重磨损，配合间隙过大，或者机油压力过高，导致机油上窜进燃烧室。而机油的渗漏主要是因为密封垫变硬老化、气门卡死。如果是老旧车辆，一般都存在密封垫由于老化而密封不严的情况。遇到以上情况，您最好是通过专业的养护中心，由养护工程师进行判定，并实施行之有效的解决办法。\")})\n",
    "response = model.chat(tokenizer, messages)\n",
    "print(\"Baichuan2: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c726a-6ab4-40f9-ae4e-d7b217891c98",
   "metadata": {},
   "source": [
    "## 5 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0069e04-1465-4bf4-83e2-8831ba5bd144",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:29:49.866757Z",
     "iopub.status.busy": "2023-09-21T15:29:49.866407Z",
     "iopub.status.idle": "2023-09-21T15:29:49.870795Z",
     "shell.execute_reply": "2023-09-21T15:29:49.870261Z",
     "shell.execute_reply.started": "2023-09-21T15:29:49.866736Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def predict_one_sample(model, tokenizer, text):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"user\", \"content\": get_prompt(text)})\n",
    "    with torch.no_grad():\n",
    "        response = model.chat(tokenizer, messages)\n",
    "    \n",
    "    # 对结果按照“\\n”进行分割，获取每个三元组内容\n",
    "    pre_res = list(set([rr for rr in response.split('\\\\n') if len(rr.split(\"_\"))==3]))\n",
    "\n",
    "    return response, pre_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c59b6f-3176-48f9-98c0-7542d4738d6a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-09-21T15:29:59.946177Z",
     "iopub.status.busy": "2023-09-21T15:29:59.945818Z",
     "iopub.status.idle": "2023-09-21T15:30:59.574771Z",
     "shell.execute_reply": "2023-09-21T15:30:59.574237Z",
     "shell.execute_reply.started": "2023-09-21T15:29:59.946156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 100%|██████████| 50/50 [00:59<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.4785412172368694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = \"dataset/test.json\"\n",
    "save_data = []\n",
    "f1, total = 0.0, 0.0\n",
    "\n",
    "test_json = json.load(open(cfg.test_path, \"r\"))\n",
    "sources = [example[\"conversations\"] for example in test_json]\n",
    "\n",
    "for i, source in enumerate(tqdm(sources, desc=\"iter\")):\n",
    "    total += 1\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = sentence[\"from\"]\n",
    "        value = sentence[\"value\"]\n",
    "\n",
    "        if role == \"user\":\n",
    "            text = value\n",
    "        else:\n",
    "            answer = value\n",
    "    \n",
    "    response, pre_res = predict_one_sample(model, tokenizer, text)\n",
    "    real_res = answer.split(\"\\\\n\")\n",
    "    \n",
    "    # 计算预测与真实的F1值\n",
    "    same_res = set(pre_res) & set(real_res)\n",
    "    if len(set(pre_res)) == 0:\n",
    "        p = 0.0\n",
    "    else:\n",
    "        p = len(same_res) / len(set(pre_res))\n",
    "    r = len(same_res) / len(set(real_res))\n",
    "    if (p + r) != 0.0:\n",
    "        f = 2 * p * r / (p + r)\n",
    "    else:\n",
    "        f = 0.0\n",
    "    f1 += f \n",
    "    save_data.append(\n",
    "        {\"text\": text, \"ori_answer\": answer, \"gen_answer\": response, \"f1\": f})\n",
    "    \n",
    "print(\"f1:\", f1 / total)\n",
    "save_path = os.path.join('baichuan2-7B-chat_lora', \"ft_answer.json\")\n",
    "fin = open(save_path, \"w\", encoding=\"utf-8\")\n",
    "json.dump(save_data, fin, ensure_ascii=False, indent=4)\n",
    "fin.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
