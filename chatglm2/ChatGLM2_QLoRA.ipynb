{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q 'bitsandbytes==0.39.1'\n",
    "# %pip install -q peft\n",
    "# %pip install -q torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "cfg = Namespace()\n",
    "\n",
    "# model\n",
    "cfg.model_name_or_path = '/home/huang/models/chatglm2-6b'\n",
    "cfg.quantization_bit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_doble_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(cfg.model_name_or_path, quantization_config=bnb_config,\n",
    "                                  trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 文字接龙接口\n",
    "text = '世界上最高的山峰是什么？'\n",
    "inputs = tokenizer(text)\n",
    "inputs = {k:torch.tensor([v]) for k,v in inputs.items()}\n",
    "outputs = model.generate(**inputs,max_new_tokens=64,repetition_penalty=1.1)\n",
    "tokenizer.batch_decode(outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat 聊天接口\n",
    "response,history= model.chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream_chat 流聊天接口(打字机风格)\n",
    "result = model.stream_chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\n",
    "for response,history in result:\n",
    "    print(response,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注册魔法命令便于jupyter中使用\n",
    "from torchkeras.chat import ChatGLM \n",
    "chatglm = ChatGLM(model,tokenizer,stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%chatglm\n",
    "你好呀，请介绍一下你自己？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%chatglm\n",
    "你听说过梦中情炉吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
